{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c32c924c",
   "metadata": {},
   "source": [
    "## Off the shelf results with T5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "928f6d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d883b7d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bf01d415b334f5eb2287ec21b65ea8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2fa8618db574403810d35ff304693ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "base_model = T5ForConditionalGeneration.from_pretrained('t5-base')\n",
    "base_tokenizer = T5Tokenizer.from_pretrained('t5-base')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c21f6b",
   "metadata": {},
   "source": [
    "## Abstractive Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd740019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original text preprocessed:\n",
      " Barack Obama is an American politician, lawyer, and author who served as the 44th President of the United States from 2009 to 2017, becoming the first African American to hold the office. Born on August 4, 1961, in Honolulu, Hawaii, he graduated from Columbia University and Harvard Law School, where he became the first Black president of the Harvard Law Review. Before his presidency, Obama worked as a community organizer, practiced civil rights law, and served as a U.S. Senator from Illinois. His presidency was marked by significant achievements such as the Affordable Care Act (Obamacare), the operation that killed Osama bin Laden, the Paris Climate Agreement, and efforts to recover from the 2008 financial crisis. Known for his eloquence and hope-driven leadership, Obama remains a prominent global figure advocating for democracy, equality, and climate action.\n"
     ]
    }
   ],
   "source": [
    "text_to_summarize =\"\"\"Barack Obama is an American politician, lawyer, and author who served as the 44th President of the United States from 2009 to 2017, \n",
    "becoming the first African American to hold the office. Born on August 4, 1961, in Honolulu, Hawaii, he graduated from Columbia University and Harvard Law \n",
    "School, where he became the first Black president of the Harvard Law Review. Before his presidency, Obama worked as a community organizer, practiced civil \n",
    "rights law, and served as a U.S. Senator from Illinois. His presidency was marked by significant achievements such as the Affordable Care Act (Obamacare), \n",
    "the operation that killed Osama bin Laden, the Paris Climate Agreement, and efforts to recover from the 2008 financial crisis. \n",
    "Known for his eloquence and hope-driven leadership, Obama remains a prominent global figure advocating for democracy, equality, and climate action.\n",
    "\"\"\"\n",
    "\n",
    "preprocess_text = text_to_summarize.strip().replace(\"\\n\",\"\")\n",
    "\n",
    "print (\"original text preprocessed:\\n\", preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b987bdf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarized text: \n",
      "born on august 4, 1961, in Honolulu, he was the first black president of the u.s. he served as the 44th president from 2009 to 2017 .\n"
     ]
    }
   ],
   "source": [
    "# known prompt for summarization with T5\n",
    "t5_prepared_text = \"summarize: \" + preprocess_text\n",
    "\n",
    "input_ids = base_tokenizer.encode(t5_prepared_text, return_tensors=\"pt\")\n",
    "\n",
    "# summmarize \n",
    "summary_ids = base_model.generate(\n",
    "    input_ids,\n",
    "    num_beams=4,\n",
    "    no_repeat_ngram_size=3,\n",
    "    min_length=30,\n",
    "    max_length=50,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "output = base_tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print (f\"Summarized text: \\n{output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba2c628",
   "metadata": {},
   "source": [
    "## English -> German Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97f956b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated text:\n",
      "Wo ist der Apfel?\n"
     ]
    }
   ],
   "source": [
    "input_ids = base_tokenizer.encode('translate English to German: Where is the apple?', return_tensors='pt')\n",
    "\n",
    "# translate \n",
    "translate_ids = base_model.generate(\n",
    "    input_ids,\n",
    "    num_beams=4,\n",
    "    no_repeat_ngram_size=3,\n",
    "    max_length=20,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "output = base_tokenizer.decode(translate_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print (f\"Translated text:\\n{output}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6bb64ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[ 3488,   229,    67, 31267,    58,     1]]),\n",
       " tensor(2.0196, grad_fn=<NllLossBackward0>))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pass labels in to calculate loss\n",
    "\n",
    "input_ids = base_tokenizer('translate English to German: Where is the apple?', return_tensors='pt').input_ids\n",
    "labels = base_tokenizer('Wo ist die Schokolade?', return_tensors='pt').input_ids\n",
    "\n",
    "loss = base_model(input_ids=input_ids, labels=labels).loss\n",
    "\n",
    "labels, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3012d5e3",
   "metadata": {},
   "source": [
    "## CoLA: The Corpus of Linguistic Acceptability\n",
    "checking for grammatical correctess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ada39336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is grammatically correct?: \n",
      "acceptable\n"
     ]
    }
   ],
   "source": [
    "input_ids = base_tokenizer.encode('cola sentence: Where is the apple?', return_tensors='pt')\n",
    "\n",
    "# CoLA \n",
    "translate_ids = base_model.generate(\n",
    "    input_ids,\n",
    "    num_beams=4,\n",
    "    no_repeat_ngram_size=3,\n",
    "    max_length=20,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "output = base_tokenizer.decode(translate_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"is grammatically correct?: \\n{output}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2200dc26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is grammatically correct?: \n",
      "unacceptable\n"
     ]
    }
   ],
   "source": [
    "input_ids = base_tokenizer.encode('cola sentence: Where be a apples?', return_tensors='pt')\n",
    "\n",
    "# summmarize \n",
    "translate_ids = base_model.generate(\n",
    "    input_ids,\n",
    "    max_length=20,\n",
    "    num_beams =2,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "output = base_tokenizer.decode(translate_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"is grammatically correct?: \\n{output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509b5967",
   "metadata": {},
   "source": [
    "## STSB - Semantic Text Similarity Benchmark\n",
    "Are two sentences semantically similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff07d3b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "semantically similar? (0-5): \n",
      "3.2\n"
     ]
    }
   ],
   "source": [
    "sentence_one = 'How to fish'\n",
    "sentence_two = 'Fishing Manual for beginnners'\n",
    "\n",
    "\n",
    "input_ids = base_tokenizer.encode(f\"stsb sentence1: {sentence_one} sentence2: {sentence_two}\", return_tensors='pt')\n",
    "\n",
    "# calculate semantic similarity \n",
    "translate_ids = base_model.generate(\n",
    "    input_ids,\n",
    "    max_length=3,\n",
    "    num_beams =3,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "output = base_tokenizer.decode(translate_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"semantically similar? (0-5): \\n{output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b361b234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "semantically similar? (0-5): \n",
      "0.4\n"
     ]
    }
   ],
   "source": [
    "sentence_one = 'How to fish'\n",
    "sentence_two = 'Hiking Manual for beginnners'\n",
    "\n",
    "\n",
    "input_ids = base_tokenizer.encode(f\"stsb sentence1: {sentence_one} sentence2: {sentence_two}\", return_tensors='pt')\n",
    "\n",
    "# calculate semantic similarity\n",
    "translate_ids = base_model.generate(\n",
    "    input_ids,\n",
    "    max_length=3,\n",
    "    num_beams =2,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "output = base_tokenizer.decode(translate_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"semantically similar? (0-5): \\n{output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3081d51",
   "metadata": {},
   "source": [
    "## MNLI - Multi-Genre Natural Language Inference\n",
    "Whether a premise implies (“entailment”), contradicts (“contradiction”), or neither (“neutral”) a hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d3808dda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: \n",
      "entailment\n"
     ]
    }
   ],
   "source": [
    "input_ids = base_tokenizer.encode(\n",
    "    'mnli premise: I am active in politics. hypothesis: I am running for mayor', return_tensors='pt'\n",
    ")\n",
    "\n",
    "# mnli \n",
    "translate_ids = base_model.generate(\n",
    "    input_ids,\n",
    "    num_beams=2,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "output = base_tokenizer.decode(translate_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Response: \\n{output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4054466d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: \n",
      "contradiction\n"
     ]
    }
   ],
   "source": [
    "input_ids = base_tokenizer.encode(\n",
    "    'mnli premise: I am active in politics. hypothesis: I do not really vote', return_tensors='pt'\n",
    ")\n",
    "\n",
    "# mnli \n",
    "translate_ids = base_model.generate(\n",
    "    input_ids,\n",
    "    early_stopping=True,\n",
    "    num_beams=3\n",
    ")\n",
    "\n",
    "output = base_tokenizer.decode(translate_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Response: \\n{output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ab54bbec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: \n",
      "neutral\n"
     ]
    }
   ],
   "source": [
    "input_ids = base_tokenizer.encode(\n",
    "    'mnli premise: I am active in politics. hypothesis: I code for a living', return_tensors='pt'\n",
    ")\n",
    "\n",
    "# mnli \n",
    "translate_ids = base_model.generate(\n",
    "    input_ids,\n",
    "    early_stopping=True,\n",
    "    num_beams=3\n",
    ")\n",
    "\n",
    "output = base_tokenizer.decode(translate_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Response: \\n{output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca923c09",
   "metadata": {},
   "source": [
    "## Q/A - Question/Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "31f2923f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: \n",
      "Hawai\n"
     ]
    }
   ],
   "source": [
    "input_ids = base_tokenizer.encode(\n",
    "    'question: Where does Obama live? context: Obama lives in Hawai but Matt lives in Boston.', return_tensors='pt'\n",
    ")\n",
    "\n",
    "# Q/A\n",
    "translate_ids = base_model.generate(\n",
    "    input_ids,\n",
    "    early_stopping=True,\n",
    "    num_beams = 2,\n",
    ")\n",
    "\n",
    "output = base_tokenizer.decode(translate_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Response: \\n{output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b0cd275e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: \n",
      "Boston\n"
     ]
    }
   ],
   "source": [
    "input_ids = base_tokenizer.encode(\n",
    "    'question: Where does Matt live? context: Obama lives in Hawai but Matt lives in Boston.', return_tensors='pt'\n",
    ")\n",
    "\n",
    "# Q/A\n",
    "translate_ids = base_model.generate(\n",
    "    input_ids,\n",
    "    early_stopping=True,\n",
    "    num_beams = 3\n",
    ")\n",
    "\n",
    "output = base_tokenizer.decode(translate_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Response: \\n{output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c20b4ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: \n",
      "prompt2\n"
     ]
    }
   ],
   "source": [
    "# Sanity check with random prompts\n",
    "\n",
    "input_ids = base_tokenizer.encode(\n",
    "    'prompt1: Where does Matt live? prompt2: Obama lives in Hawai but Matt lives in Boston.', return_tensors='pt'\n",
    ")\n",
    "\n",
    "# Q/A\n",
    "translate_ids = base_model.generate(\n",
    "    input_ids,\n",
    "    early_stopping=True,\n",
    "    num_beams =3\n",
    ")\n",
    "\n",
    "output = base_tokenizer.decode(translate_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Response: \\n{output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336c91b9",
   "metadata": {},
   "source": [
    "## Using T5 for abstractive summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8f4fcdff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, T5ForConditionalGeneration, TrainingArguments, Trainer, \\\n",
    "                         DataCollatorForSeq2Seq\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ceea57f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "beaa6594217d406ebc57c9da95ea3a03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9fbdb2228704ed49ac04265611e541c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ed66f8f977d4773a8cc8b8659d72309",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cad415ae9a2b4a0494bb844940aea8b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "531cad201bba4348bee8412b14111e41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de80eb679c4d4d16a83d66d63e07230d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "base_model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
    "base_tokenizer = T5Tokenizer.from_pretrained('t5-small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "33f3a084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(96486, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "      <td>Great taffy.</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This taffy is so good.  It is very soft and ch...</td>\n",
       "      <td>Wonderful, tasty taffy.</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Right now I'm mostly just sprouting this so my...</td>\n",
       "      <td>Yay Barley.</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This is a very healthy dog food. Good for thei...</td>\n",
       "      <td>Healthy Dog Food.</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>good flavor! these came securely packed... the...</td>\n",
       "      <td>fresh and greasy!</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text                  Summary  \\\n",
       "0  Great taffy at a great price.  There was a wid...             Great taffy.   \n",
       "1  This taffy is so good.  It is very soft and ch...  Wonderful, tasty taffy.   \n",
       "2  Right now I'm mostly just sprouting this so my...              Yay Barley.   \n",
       "3  This is a very healthy dog food. Good for thei...        Healthy Dog Food.   \n",
       "4  good flavor! these came securely packed... the...        fresh and greasy!   \n",
       "\n",
       "   Score  \n",
       "0      5  \n",
       "1      5  \n",
       "2      5  \n",
       "3      5  \n",
       "4      4  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://www.kaggle.com/snap/amazon-fine-food-reviews?select=Reviews.csv\n",
    "\n",
    "reviews = pd.read_csv('data/reviews.csv')\n",
    "\n",
    "# Pre-processing step\n",
    "# Punctuation is important in grammar and important for complex decoding architectures to know when to stop!\n",
    "def add_punc(s):\n",
    "    if s[-1] not in ('.', '!', '?'):\n",
    "        s = s + '.'\n",
    "    return s\n",
    "\n",
    "reviews.dropna(inplace=True)\n",
    "\n",
    "reviews['Summary'] = reviews['Summary'].map(add_punc)\n",
    "\n",
    "print(reviews.shape)\n",
    "\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "da4c1dcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13073, 3)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews = reviews[(reviews['Summary'].str.len() < 100) & (reviews['Summary'].str.len() >=30)]\n",
    "\n",
    "reviews.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "41ba36f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "\n",
    "reviews_dataset = Dataset.from_pandas(reviews.astype(str).sample(5000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6e4aff70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have a prompt but only as a prefix in the encoder\n",
    "prefix = \"summarize: \"\n",
    "\n",
    "# we will manually add our own labels because unlike GPT, we cannot assume the labels are based on the inputs\n",
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + doc for doc in examples[\"Text\"]]\n",
    "    model_inputs = base_tokenizer(inputs, max_length=1024, truncation=True)\n",
    "\n",
    "    labels = base_tokenizer(examples[\"Summary\"], max_length=128, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "822208b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd8573839fa247018408db5967f06a29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_reviews_dataset = reviews_dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e71d418e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_reviews_dataset = tokenized_reviews_dataset.train_test_split(test_size=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ca5565f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data collator specifically for generic sequence to sequence tasks\n",
    "# Use when we are translating one sequence to another like translation, summarization, etc\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=base_tokenizer, model=base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dd24d2d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='16' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 6/16 00:00 < 00:00, 44.78 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 4.3676838874816895,\n",
       " 'eval_model_preparation_time': 0.0026,\n",
       " 'eval_runtime': 0.3665,\n",
       " 'eval_samples_per_second': 1364.173,\n",
       " 'eval_steps_per_second': 43.654}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./t5_summary_results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=20,\n",
    "    load_best_model_at_end=True,\n",
    "    logging_steps=50,\n",
    "    save_strategy='epoch'\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=base_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_reviews_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_reviews_dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2f1671d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2820' max='2820' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2820/2820 03:38, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Model Preparation Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.701200</td>\n",
       "      <td>3.299554</td>\n",
       "      <td>0.002600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.513300</td>\n",
       "      <td>3.229542</td>\n",
       "      <td>0.002600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.444200</td>\n",
       "      <td>3.170504</td>\n",
       "      <td>0.002600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.299200</td>\n",
       "      <td>3.132667</td>\n",
       "      <td>0.002600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.256500</td>\n",
       "      <td>3.101396</td>\n",
       "      <td>0.002600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.203100</td>\n",
       "      <td>3.071202</td>\n",
       "      <td>0.002600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>3.209100</td>\n",
       "      <td>3.053885</td>\n",
       "      <td>0.002600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>3.149200</td>\n",
       "      <td>3.035452</td>\n",
       "      <td>0.002600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>3.110900</td>\n",
       "      <td>3.021343</td>\n",
       "      <td>0.002600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.121500</td>\n",
       "      <td>3.007421</td>\n",
       "      <td>0.002600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>3.087400</td>\n",
       "      <td>2.997882</td>\n",
       "      <td>0.002600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>3.052900</td>\n",
       "      <td>2.987408</td>\n",
       "      <td>0.002600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>3.033900</td>\n",
       "      <td>2.979537</td>\n",
       "      <td>0.002600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>2.985100</td>\n",
       "      <td>2.976773</td>\n",
       "      <td>0.002600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>2.950600</td>\n",
       "      <td>2.971657</td>\n",
       "      <td>0.002600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>3.003100</td>\n",
       "      <td>2.966610</td>\n",
       "      <td>0.002600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>2.940300</td>\n",
       "      <td>2.962434</td>\n",
       "      <td>0.002600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>2.968000</td>\n",
       "      <td>2.959196</td>\n",
       "      <td>0.002600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>2.965000</td>\n",
       "      <td>2.958310</td>\n",
       "      <td>0.002600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.936400</td>\n",
       "      <td>2.958154</td>\n",
       "      <td>0.002600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2820, training_loss=3.15647518617887, metrics={'train_runtime': 218.4289, 'train_samples_per_second': 412.033, 'train_steps_per_second': 12.91, 'total_flos': 1246856435859456.0, 'train_loss': 3.15647518617887, 'epoch': 20.0})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "54510733",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16' max='16' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16/16 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 2.95815372467041,\n",
       " 'eval_model_preparation_time': 0.0026,\n",
       " 'eval_runtime': 0.3524,\n",
       " 'eval_samples_per_second': 1418.784,\n",
       " 'eval_steps_per_second': 45.401,\n",
       " 'epoch': 20.0}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "221a71f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6ab1d16d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "loaded_model = T5ForConditionalGeneration.from_pretrained('./t5_summary_results')\n",
    "\n",
    "# summarization pipeline prepends a default prefix of summarize: \n",
    "generator = pipeline(\n",
    "    'summarization', model=loaded_model, tokenizer=base_tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ad6b8780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74105    Great Coffee at a great value.\n",
      "Name: Summary, dtype: object\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I am a coffee fanatic.  This coffee is delicious!  As good if not better than other brands, and the price is reasonable.'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sam = reviews.sample(1)\n",
    "print(sam['Summary'])\n",
    "text = sam['Text'].tolist()[0]\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c325f6c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary_text': 'Great coffee for a great price.'}]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate a summary\n",
    "generator(text, min_length=3, max_length=15, early_stopping=True, num_beams=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c7d83a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'summary_text': 'a coffee fanatic, this coffee is delicious!'}]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try the base t5 on the same text\n",
    "base_generator = pipeline(\n",
    "    'summarization', model='t5-small', tokenizer='t5-small'\n",
    ")\n",
    "\n",
    "# Summary is a bit more extractive than our fine-tuned version and style isn't quite the same as our dataset\n",
    "base_generator(text, min_length=3, max_length=15, early_stopping=True, num_beams=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4c8acb38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yummy, but not my prompt.\n"
     ]
    }
   ],
   "source": [
    "# Sanity check: trying a different prefix. Not a good result\n",
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "inputs = base_tokenizer(\"not my prompt: \" + text, return_tensors=\"pt\")\n",
    "inputs = inputs.to(device)\n",
    "outputs = loaded_model.generate(\n",
    "    inputs[\"input_ids\"], min_length=3, max_length=15,\n",
    ")\n",
    "\n",
    "print(base_tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TransformNLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
